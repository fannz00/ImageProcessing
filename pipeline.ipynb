{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmenter import run_segmenter\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from pandas.errors import EmptyDataError\n",
    "import numpy as np\n",
    "import shutil\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19339/19339 [00:49<00:00, 393.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files were empty and were dropped; Number of uniue images: 13244\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pressure [dbar]</th>\n",
       "      <th>temperature</th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>img_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>mean_raw</th>\n",
       "      <th>std_raw</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>object_boundbox_area</th>\n",
       "      <th>object_eccentricity</th>\n",
       "      <th>object_equivalent_diameter</th>\n",
       "      <th>object_euler_nr</th>\n",
       "      <th>object_extent</th>\n",
       "      <th>object_local_centroid_col</th>\n",
       "      <th>object_local_centroid_row</th>\n",
       "      <th>object_solidity</th>\n",
       "      <th>full_path</th>\n",
       "      <th>esd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>29.97</td>\n",
       "      <td>16</td>\n",
       "      <td>20220505</td>\n",
       "      <td>03025559</td>\n",
       "      <td>1</td>\n",
       "      <td>20220505-03025559_001.001bar_29.97C_16.png</td>\n",
       "      <td>95.101949</td>\n",
       "      <td>65.612728</td>\n",
       "      <td>108.226437</td>\n",
       "      <td>...</td>\n",
       "      <td>3717.0</td>\n",
       "      <td>0.644103</td>\n",
       "      <td>60.186099</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.765402</td>\n",
       "      <td>31.348682</td>\n",
       "      <td>27.876977</td>\n",
       "      <td>0.957590</td>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>1597.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>29.97</td>\n",
       "      <td>17</td>\n",
       "      <td>20220505</td>\n",
       "      <td>03025559</td>\n",
       "      <td>1</td>\n",
       "      <td>20220505-03025559_001.001bar_29.97C_17.png</td>\n",
       "      <td>95.101949</td>\n",
       "      <td>65.612728</td>\n",
       "      <td>108.226437</td>\n",
       "      <td>...</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0.291325</td>\n",
       "      <td>37.830163</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.759459</td>\n",
       "      <td>20.257117</td>\n",
       "      <td>17.767794</td>\n",
       "      <td>0.945332</td>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>994.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>29.97</td>\n",
       "      <td>18</td>\n",
       "      <td>20220505</td>\n",
       "      <td>03025559</td>\n",
       "      <td>1</td>\n",
       "      <td>20220505-03025559_001.001bar_29.97C_18.png</td>\n",
       "      <td>95.101949</td>\n",
       "      <td>65.612728</td>\n",
       "      <td>108.226437</td>\n",
       "      <td>...</td>\n",
       "      <td>20276.0</td>\n",
       "      <td>0.976962</td>\n",
       "      <td>111.000626</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.477264</td>\n",
       "      <td>122.772140</td>\n",
       "      <td>45.303296</td>\n",
       "      <td>0.497021</td>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>2840.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>29.97</td>\n",
       "      <td>23</td>\n",
       "      <td>20220505</td>\n",
       "      <td>03025559</td>\n",
       "      <td>1</td>\n",
       "      <td>20220505-03025559_001.001bar_29.97C_23.png</td>\n",
       "      <td>95.101949</td>\n",
       "      <td>65.612728</td>\n",
       "      <td>108.226437</td>\n",
       "      <td>...</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>0.899237</td>\n",
       "      <td>26.221162</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.435484</td>\n",
       "      <td>20.968519</td>\n",
       "      <td>17.824074</td>\n",
       "      <td>0.707733</td>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>658.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>29.97</td>\n",
       "      <td>32</td>\n",
       "      <td>20220505</td>\n",
       "      <td>03025559</td>\n",
       "      <td>1</td>\n",
       "      <td>20220505-03025559_001.001bar_29.97C_32.png</td>\n",
       "      <td>95.101949</td>\n",
       "      <td>65.612728</td>\n",
       "      <td>108.226437</td>\n",
       "      <td>...</td>\n",
       "      <td>600.0</td>\n",
       "      <td>0.276787</td>\n",
       "      <td>24.042686</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.756667</td>\n",
       "      <td>11.143172</td>\n",
       "      <td>11.828194</td>\n",
       "      <td>0.949791</td>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>623.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111253</th>\n",
       "      <td>4276.12</td>\n",
       "      <td>2.14</td>\n",
       "      <td>660</td>\n",
       "      <td>20220505</td>\n",
       "      <td>04235560</td>\n",
       "      <td>19339</td>\n",
       "      <td>20220505-04235560_428.612bar_02.14C_660.png</td>\n",
       "      <td>167.868812</td>\n",
       "      <td>34.487374</td>\n",
       "      <td>1.942304</td>\n",
       "      <td>...</td>\n",
       "      <td>3192.0</td>\n",
       "      <td>0.569987</td>\n",
       "      <td>38.464326</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.364035</td>\n",
       "      <td>27.527539</td>\n",
       "      <td>27.420826</td>\n",
       "      <td>0.481558</td>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>965.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111254</th>\n",
       "      <td>4276.12</td>\n",
       "      <td>2.14</td>\n",
       "      <td>667</td>\n",
       "      <td>20220505</td>\n",
       "      <td>04235560</td>\n",
       "      <td>19339</td>\n",
       "      <td>20220505-04235560_428.612bar_02.14C_667.png</td>\n",
       "      <td>167.868812</td>\n",
       "      <td>34.487374</td>\n",
       "      <td>1.942304</td>\n",
       "      <td>...</td>\n",
       "      <td>4128.0</td>\n",
       "      <td>0.960751</td>\n",
       "      <td>48.834185</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.453731</td>\n",
       "      <td>20.106247</td>\n",
       "      <td>50.789108</td>\n",
       "      <td>0.722887</td>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>1273.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111255</th>\n",
       "      <td>4276.12</td>\n",
       "      <td>2.14</td>\n",
       "      <td>683</td>\n",
       "      <td>20220505</td>\n",
       "      <td>04235560</td>\n",
       "      <td>19339</td>\n",
       "      <td>20220505-04235560_428.612bar_02.14C_683.png</td>\n",
       "      <td>167.868812</td>\n",
       "      <td>34.487374</td>\n",
       "      <td>1.942304</td>\n",
       "      <td>...</td>\n",
       "      <td>6762.0</td>\n",
       "      <td>0.970612</td>\n",
       "      <td>61.120249</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.433895</td>\n",
       "      <td>24.666667</td>\n",
       "      <td>63.794819</td>\n",
       "      <td>0.570595</td>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>1579.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111256</th>\n",
       "      <td>4276.12</td>\n",
       "      <td>2.14</td>\n",
       "      <td>763</td>\n",
       "      <td>20220505</td>\n",
       "      <td>04235560</td>\n",
       "      <td>19339</td>\n",
       "      <td>20220505-04235560_428.612bar_02.14C_763.png</td>\n",
       "      <td>167.868812</td>\n",
       "      <td>34.487374</td>\n",
       "      <td>1.942304</td>\n",
       "      <td>...</td>\n",
       "      <td>1624.0</td>\n",
       "      <td>0.940013</td>\n",
       "      <td>29.185566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.411946</td>\n",
       "      <td>15.860987</td>\n",
       "      <td>32.390135</td>\n",
       "      <td>0.669000</td>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>738.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111257</th>\n",
       "      <td>4276.12</td>\n",
       "      <td>2.14</td>\n",
       "      <td>795</td>\n",
       "      <td>20220505</td>\n",
       "      <td>04235560</td>\n",
       "      <td>19339</td>\n",
       "      <td>20220505-04235560_428.612bar_02.14C_795.png</td>\n",
       "      <td>167.868812</td>\n",
       "      <td>34.487374</td>\n",
       "      <td>1.942304</td>\n",
       "      <td>...</td>\n",
       "      <td>1425.0</td>\n",
       "      <td>0.937968</td>\n",
       "      <td>32.566985</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.584561</td>\n",
       "      <td>11.105642</td>\n",
       "      <td>27.404562</td>\n",
       "      <td>0.787335</td>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>837.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111258 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pressure [dbar]  temperature  index      date      time  img_id  \\\n",
       "0                  0.01        29.97     16  20220505  03025559       1   \n",
       "1                  0.01        29.97     17  20220505  03025559       1   \n",
       "2                  0.01        29.97     18  20220505  03025559       1   \n",
       "3                  0.01        29.97     23  20220505  03025559       1   \n",
       "4                  0.01        29.97     32  20220505  03025559       1   \n",
       "...                 ...          ...    ...       ...       ...     ...   \n",
       "111253          4276.12         2.14    660  20220505  04235560   19339   \n",
       "111254          4276.12         2.14    667  20220505  04235560   19339   \n",
       "111255          4276.12         2.14    683  20220505  04235560   19339   \n",
       "111256          4276.12         2.14    763  20220505  04235560   19339   \n",
       "111257          4276.12         2.14    795  20220505  04235560   19339   \n",
       "\n",
       "                                           filename    mean_raw    std_raw  \\\n",
       "0        20220505-03025559_001.001bar_29.97C_16.png   95.101949  65.612728   \n",
       "1        20220505-03025559_001.001bar_29.97C_17.png   95.101949  65.612728   \n",
       "2        20220505-03025559_001.001bar_29.97C_18.png   95.101949  65.612728   \n",
       "3        20220505-03025559_001.001bar_29.97C_23.png   95.101949  65.612728   \n",
       "4        20220505-03025559_001.001bar_29.97C_32.png   95.101949  65.612728   \n",
       "...                                             ...         ...        ...   \n",
       "111253  20220505-04235560_428.612bar_02.14C_660.png  167.868812  34.487374   \n",
       "111254  20220505-04235560_428.612bar_02.14C_667.png  167.868812  34.487374   \n",
       "111255  20220505-04235560_428.612bar_02.14C_683.png  167.868812  34.487374   \n",
       "111256  20220505-04235560_428.612bar_02.14C_763.png  167.868812  34.487374   \n",
       "111257  20220505-04235560_428.612bar_02.14C_795.png  167.868812  34.487374   \n",
       "\n",
       "              mean  ...  object_boundbox_area  object_eccentricity  \\\n",
       "0       108.226437  ...                3717.0             0.644103   \n",
       "1       108.226437  ...                1480.0             0.291325   \n",
       "2       108.226437  ...               20276.0             0.976962   \n",
       "3       108.226437  ...                1240.0             0.899237   \n",
       "4       108.226437  ...                 600.0             0.276787   \n",
       "...            ...  ...                   ...                  ...   \n",
       "111253    1.942304  ...                3192.0             0.569987   \n",
       "111254    1.942304  ...                4128.0             0.960751   \n",
       "111255    1.942304  ...                6762.0             0.970612   \n",
       "111256    1.942304  ...                1624.0             0.940013   \n",
       "111257    1.942304  ...                1425.0             0.937968   \n",
       "\n",
       "        object_equivalent_diameter  object_euler_nr  object_extent  \\\n",
       "0                        60.186099              1.0       0.765402   \n",
       "1                        37.830163              1.0       0.759459   \n",
       "2                       111.000626              2.0       0.477264   \n",
       "3                        26.221162              1.0       0.435484   \n",
       "4                        24.042686              1.0       0.756667   \n",
       "...                            ...              ...            ...   \n",
       "111253                   38.464326              1.0       0.364035   \n",
       "111254                   48.834185              1.0       0.453731   \n",
       "111255                   61.120249              1.0       0.433895   \n",
       "111256                   29.185566              1.0       0.411946   \n",
       "111257                   32.566985              1.0       0.584561   \n",
       "\n",
       "        object_local_centroid_col  object_local_centroid_row  object_solidity  \\\n",
       "0                       31.348682                  27.876977         0.957590   \n",
       "1                       20.257117                  17.767794         0.945332   \n",
       "2                      122.772140                  45.303296         0.497021   \n",
       "3                       20.968519                  17.824074         0.707733   \n",
       "4                       11.143172                  11.828194         0.949791   \n",
       "...                           ...                        ...              ...   \n",
       "111253                  27.527539                  27.420826         0.481558   \n",
       "111254                  20.106247                  50.789108         0.722887   \n",
       "111255                  24.666667                  63.794819         0.570595   \n",
       "111256                  15.860987                  32.390135         0.669000   \n",
       "111257                  11.105642                  27.404562         0.787335   \n",
       "\n",
       "                                                full_path      esd  \n",
       "0       /home/fanny/M181-2_output_test/M181-117-1_CTD-...  1597.95  \n",
       "1       /home/fanny/M181-2_output_test/M181-117-1_CTD-...   994.25  \n",
       "2       /home/fanny/M181-2_output_test/M181-117-1_CTD-...  2840.31  \n",
       "3       /home/fanny/M181-2_output_test/M181-117-1_CTD-...   658.03  \n",
       "4       /home/fanny/M181-2_output_test/M181-117-1_CTD-...   623.26  \n",
       "...                                                   ...      ...  \n",
       "111253  /home/fanny/M181-2_output_test/M181-117-1_CTD-...   965.35  \n",
       "111254  /home/fanny/M181-2_output_test/M181-117-1_CTD-...  1273.04  \n",
       "111255  /home/fanny/M181-2_output_test/M181-117-1_CTD-...  1579.55  \n",
       "111256  /home/fanny/M181-2_output_test/M181-117-1_CTD-...   738.14  \n",
       "111257  /home/fanny/M181-2_output_test/M181-117-1_CTD-...   837.96  \n",
       "\n",
       "[111258 rows x 51 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_crop_df(path:str, small:bool, size_filter:int = 0):\n",
    "    \"\"\"\n",
    "    A function to generate a DataFrame from a directory of CSV files, with options to filter out small objects.\n",
    "    Parameters:\n",
    "    path (str): The path to the directory containing the CSV files.\n",
    "    small (bool): A flag indicating whether to filter out small objects.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The concatenated and processed DataFrame with additional columns for analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def area_to_esd(area: float) -> float:\n",
    "        pixel_size = 13.5*2 #in µm/pixel @ 2560x2560 \n",
    "        return 2 * np.sqrt(area * pixel_size**2 / np.pi)\n",
    "\n",
    "    # Function to concatenate directory and filename\n",
    "    def join_strings(dir, filename):\n",
    "        return os.path.join(dir, filename)\n",
    "\n",
    "    directory = os.path.dirname(path)\n",
    "    directory = os.path.join(directory,'Data')\n",
    "\n",
    "    files = [os.path.join(path, file) for file in sorted(os.listdir(path)) if file.endswith(\".csv\")]\n",
    "    dataframes = []\n",
    "    empty_file_counter = 0\n",
    "    id = 1\n",
    "    for file in tqdm(files):\n",
    "        try:\n",
    "            df = pd.read_csv(file, delimiter=\",\", header=None, index_col=None)\n",
    "            if len(df.columns) == 44:\n",
    "                df.insert(0,'',id)            \n",
    "                dataframes.append(df)\n",
    "                id+=1\n",
    "            else:\n",
    "                continue\n",
    "        except EmptyDataError:\n",
    "            empty_file_counter += 1\n",
    "            print(f\"File {file} is empty\")\n",
    "\n",
    "    df = pd.concat(dataframes, ignore_index=True)\n",
    "    headers = [\"img_id\",\"index\", \"filename\", \"mean_raw\", \"std_raw\", \"mean\", \"std\", \"area\", \"x\", \"y\", \"w\", \"h\", \n",
    "               \"saved\", \"object_bound_box_w\", \"object_bound_box_h\", \"bound_box_x\", \"bound_box_y\", \"object_circularity\", \"object_area_exc\", \n",
    "               \"object_area_rprops\", \"object_%area\", \"object_major_axis_len\", \"object_minor_axis_len\", \"object_centroid_y\", \"object_centroid_x\", \n",
    "               \"object_convex_area\", \"object_min_intensity\", \"object_max_intensity\", \"object_mean_intensity\", \"object_int_density\", \"object_perimeter\", \n",
    "               \"object_elongation\", \"object_range\", \"object_perim_area_excl\", \"object_perim_major\", \"object_circularity_area_excl\", \"object_angle\", \n",
    "               \"object_boundbox_area\", \"object_eccentricity\", \"object_equivalent_diameter\", \"object_euler_nr\", \"object_extent\", \n",
    "               \"object_local_centroid_col\", \"object_local_centroid_row\", \"object_solidity\"\n",
    "]\n",
    "    df.columns = headers\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.drop(\"index\", axis=1, inplace=True)\n",
    "\n",
    "    if not small:\n",
    "        df = df[df[\"saved\"] == 1]\n",
    "    df_unique = df.drop_duplicates(subset=['img_id'])\n",
    "    \n",
    "    #df.drop(\"saved\", axis=1, inplace=True)\n",
    "\n",
    "    # Split the 'filename' column\n",
    "    split_df = df['filename'].str.split('_', expand=True)\n",
    "    if small:# bug fix for segmenter where small objects are saved with _mask.png extension instead of .png: needs to be fixed if segmenter is fixed\n",
    "        headers = [\"date\", \"time\", \"pressure\", \"temperature\", \"index\", \"mask_ext\"]\n",
    "        split_df.columns = headers\n",
    "        split_df.drop(\"mask_ext\", axis=1, inplace=True)\n",
    "    else:\n",
    "        headers = [\"date-time\", \"pressure\", \"temperature\", \"index\"]#, 'drop']\n",
    "        split_df.columns = headers\n",
    "        #split_df.drop(\"drop\", axis=1, inplace=True)\n",
    "    \n",
    "    # split date-time\n",
    "    split_df[['date', 'time']] = split_df['date-time'].str.split('-', expand=True)\n",
    "    split_df.drop(columns=['date-time'], inplace=True)\n",
    "\n",
    "    split_df['pressure'] = split_df['pressure'].str.replace('bar', '', regex=False).astype(float)\n",
    "    split_df['temperature'] = split_df['temperature'].str.replace('C', '', regex=False).astype(float)\n",
    "    split_df['index'] = split_df['index'].str.replace('.png', '', regex=False).astype(int)\n",
    "    \n",
    "    # Concatenate the new columns with the original DataFrame\n",
    "    df = pd.concat([split_df, df], axis=1)\n",
    "\n",
    "    # Extend the original 'filename' column\n",
    "    df['full_path'] = df.apply(lambda x: join_strings(directory, x['filename']), axis=1)\n",
    "    #df = df.drop('filename', axis=1)\n",
    "\n",
    "    df['esd'] = df['area'].apply(area_to_esd).round(2)\n",
    "    df['pressure'] = (df['pressure']-1)*10\n",
    "    df.rename(columns={'pressure': 'pressure [dbar]'}, inplace=True)\n",
    "\n",
    "    # Sort the DataFrame by the 'date-time' column\n",
    "    df = df.sort_values(by=['date', 'time','index'], ascending=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #filter the df for objects where 1 dimension is larger than ca. 1mm\n",
    "    df = df[(df['w'] > size_filter) | (df['h'] > size_filter)]\n",
    "    df_unique = df.drop_duplicates(subset=['img_id'])\n",
    "    print(f'{empty_file_counter} files were empty and were dropped; Number of uniue images: {len(df_unique)}')\n",
    "\n",
    "    return df\n",
    "\n",
    "file_path = '/home/fanny/M181-2_output_test/M181-117-1_CTD-038_00°00S-009°00W_20220505-0250/Data'\n",
    "segmentation_df = gen_crop_df(file_path, False)\n",
    "segmentation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>top1</th>\n",
       "      <th>top2</th>\n",
       "      <th>top3</th>\n",
       "      <th>top4</th>\n",
       "      <th>top5</th>\n",
       "      <th>prob1</th>\n",
       "      <th>prob2</th>\n",
       "      <th>prob3</th>\n",
       "      <th>prob4</th>\n",
       "      <th>prob5</th>\n",
       "      <th>object_annotation_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Unknowns</td>\n",
       "      <td>Fecal pellets</td>\n",
       "      <td>0.424959</td>\n",
       "      <td>0.235316</td>\n",
       "      <td>0.210945</td>\n",
       "      <td>0.062445</td>\n",
       "      <td>0.027407</td>\n",
       "      <td>predicted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>Physonectae</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Doliolida</td>\n",
       "      <td>Salpidae</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>0.822883</td>\n",
       "      <td>0.046476</td>\n",
       "      <td>0.035859</td>\n",
       "      <td>0.013201</td>\n",
       "      <td>0.008981</td>\n",
       "      <td>predicted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Calanoida</td>\n",
       "      <td>Artefact</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>0.950195</td>\n",
       "      <td>0.004265</td>\n",
       "      <td>0.004085</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>0.003041</td>\n",
       "      <td>predicted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Fecal pellets</td>\n",
       "      <td>Radiolaria</td>\n",
       "      <td>0.587833</td>\n",
       "      <td>0.381747</td>\n",
       "      <td>0.010812</td>\n",
       "      <td>0.003838</td>\n",
       "      <td>0.002334</td>\n",
       "      <td>predicted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Unknowns</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Unknowns</td>\n",
       "      <td>0.850994</td>\n",
       "      <td>0.057319</td>\n",
       "      <td>0.035589</td>\n",
       "      <td>0.012789</td>\n",
       "      <td>0.011884</td>\n",
       "      <td>predicted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111232</th>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Unknowns</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Doliolida</td>\n",
       "      <td>0.619663</td>\n",
       "      <td>0.162431</td>\n",
       "      <td>0.141514</td>\n",
       "      <td>0.013338</td>\n",
       "      <td>0.010430</td>\n",
       "      <td>predicted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111233</th>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Tunicata</td>\n",
       "      <td>Unknowns</td>\n",
       "      <td>Unknowns</td>\n",
       "      <td>0.741055</td>\n",
       "      <td>0.048907</td>\n",
       "      <td>0.041340</td>\n",
       "      <td>0.030568</td>\n",
       "      <td>0.022196</td>\n",
       "      <td>predicted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111234</th>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Unknowns</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Protista</td>\n",
       "      <td>0.681353</td>\n",
       "      <td>0.279987</td>\n",
       "      <td>0.024871</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>predicted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111235</th>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Unknowns</td>\n",
       "      <td>Radiolaria</td>\n",
       "      <td>0.923617</td>\n",
       "      <td>0.028933</td>\n",
       "      <td>0.013302</td>\n",
       "      <td>0.011292</td>\n",
       "      <td>0.005039</td>\n",
       "      <td>predicted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111236</th>\n",
       "      <td>/home/fanny/M181-2_output_test/M181-117-1_CTD-...</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Unknowns</td>\n",
       "      <td>Detritus</td>\n",
       "      <td>Unknowns</td>\n",
       "      <td>0.522780</td>\n",
       "      <td>0.351538</td>\n",
       "      <td>0.095945</td>\n",
       "      <td>0.005059</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>predicted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111237 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 filename         top1  \\\n",
       "0       /home/fanny/M181-2_output_test/M181-117-1_CTD-...     Detritus   \n",
       "1       /home/fanny/M181-2_output_test/M181-117-1_CTD-...  Physonectae   \n",
       "2       /home/fanny/M181-2_output_test/M181-117-1_CTD-...     Detritus   \n",
       "3       /home/fanny/M181-2_output_test/M181-117-1_CTD-...     Detritus   \n",
       "4       /home/fanny/M181-2_output_test/M181-117-1_CTD-...     Detritus   \n",
       "...                                                   ...          ...   \n",
       "111232  /home/fanny/M181-2_output_test/M181-117-1_CTD-...     Detritus   \n",
       "111233  /home/fanny/M181-2_output_test/M181-117-1_CTD-...     Detritus   \n",
       "111234  /home/fanny/M181-2_output_test/M181-117-1_CTD-...     Detritus   \n",
       "111235  /home/fanny/M181-2_output_test/M181-117-1_CTD-...     Detritus   \n",
       "111236  /home/fanny/M181-2_output_test/M181-117-1_CTD-...     Detritus   \n",
       "\n",
       "             top2       top3           top4           top5     prob1  \\\n",
       "0        Detritus   Detritus       Unknowns  Fecal pellets  0.424959   \n",
       "1        Detritus  Doliolida       Salpidae       Detritus  0.822883   \n",
       "2       Calanoida   Artefact       Detritus       Detritus  0.950195   \n",
       "3        Detritus   Detritus  Fecal pellets     Radiolaria  0.587833   \n",
       "4        Detritus   Unknowns       Detritus       Unknowns  0.850994   \n",
       "...           ...        ...            ...            ...       ...   \n",
       "111232   Detritus   Unknowns       Detritus      Doliolida  0.619663   \n",
       "111233   Detritus   Tunicata       Unknowns       Unknowns  0.741055   \n",
       "111234   Detritus   Unknowns       Detritus       Protista  0.681353   \n",
       "111235   Detritus   Detritus       Unknowns     Radiolaria  0.923617   \n",
       "111236   Detritus   Unknowns       Detritus       Unknowns  0.522780   \n",
       "\n",
       "           prob2     prob3     prob4     prob5 object_annotation_status  \n",
       "0       0.235316  0.210945  0.062445  0.027407                predicted  \n",
       "1       0.046476  0.035859  0.013201  0.008981                predicted  \n",
       "2       0.004265  0.004085  0.003793  0.003041                predicted  \n",
       "3       0.381747  0.010812  0.003838  0.002334                predicted  \n",
       "4       0.057319  0.035589  0.012789  0.011884                predicted  \n",
       "...          ...       ...       ...       ...                      ...  \n",
       "111232  0.162431  0.141514  0.013338  0.010430                predicted  \n",
       "111233  0.048907  0.041340  0.030568  0.022196                predicted  \n",
       "111234  0.279987  0.024871  0.002106  0.001889                predicted  \n",
       "111235  0.028933  0.013302  0.011292  0.005039                predicted  \n",
       "111236  0.351538  0.095945  0.005059  0.002364                predicted  \n",
       "\n",
       "[111237 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare prediction data\n",
    "import re\n",
    "\n",
    "prediction_df = pd.read_csv(\"/home/fanny/M181-2_output_test/M181-117-1_CTD-038_00°00S-009°00W_20220505-0250/ViT_predictions.csv\")\n",
    "polytaxo_classes_df = pd.read_csv('/home/fanny/taxonomic_data/Polytaxo_classes.csv', sep=\"\\t\")\n",
    "\n",
    "print(len(polytaxo_classes_df.columns))\n",
    "\n",
    "prediction_df['object_annotation_status'] = 'predicted'\n",
    "\n",
    "mapping_dict = dict(zip(polytaxo_classes_df[\"Dataset Class NamePolyTaxo Description\"], polytaxo_classes_df[\"PolyTaxo Description\"]))\n",
    "columns_to_replace = [\"top1\", \"top2\", \"top3\", \"top4\", \"top5\"]\n",
    "\n",
    "# Define regex pattern to split on space, semicolon, colon, or slash\n",
    "split_pattern = r\"[ ;:/]\"\n",
    "\n",
    "'''\n",
    "#prediction_df[columns_to_replace] = prediction_df[columns_to_replace].replace(mapping_dict)\n",
    "prediction_df[columns_to_replace] = (\n",
    "    prediction_df[columns_to_replace]\n",
    "    .replace(mapping_dict)\n",
    "    .applymap(lambda x: re.split(split_pattern, str(x))[0] if pd.notna(x) else x)\n",
    ")\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Replace values and extract only the first word\n",
    "prediction_df[columns_to_replace] = prediction_df[columns_to_replace].replace(mapping_dict).apply(\n",
    "    lambda col: col.astype(str).apply(lambda x: re.split(split_pattern, x)[0] if pd.notna(x) else x)\n",
    ")\n",
    "\n",
    "prediction_df[columns_to_replace ]\n",
    "for row\n",
    "'''\n",
    "\n",
    "\n",
    "# Replace values, extract first word, and replace underscores with spaces\n",
    "prediction_df[columns_to_replace] = prediction_df[columns_to_replace].replace(mapping_dict).apply(\n",
    "    lambda col: col.astype(str).apply(\n",
    "        lambda x: re.split(split_pattern, x)[0].replace(\"_\", \" \") if pd.notna(x) else x\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try something else\n",
    "#taxoexport_df = pd.read_csv(\"/home/fanny/taxonomic_data/taxoexport_20250212_140806.tsv\")\n",
    "\n",
    "# Find common categories\n",
    "#common_categories = set(polytaxo_classes_df['Dataset Class NamePolyTaxo Description']).intersection(set(taxoexport_df['display_name']))\n",
    "#print(\"Direct matches found:\", len(common_categories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n",
      "/tmp/ipykernel_271706/3782526024.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[f]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  segm_and_prediction_df.loc[0] = dtype_row\n"
     ]
    }
   ],
   "source": [
    "# combine segmentation data frame with prediction data frame\n",
    "# Sort both DataFrames by 'filename'\n",
    "segmentation_df_sorted = segmentation_df.sort_values(by='filename').reset_index(drop=True)\n",
    "prediction_df_sorted = prediction_df.sort_values(by='filename').reset_index(drop=True)\n",
    "\n",
    "# concatenate data frames\n",
    "segm_and_prediction_df = pd.concat([segmentation_df_sorted, prediction_df_sorted], axis=1)\n",
    "segm_and_prediction_df = segm_and_prediction_df.loc[:, ~segm_and_prediction_df.columns.duplicated(keep='first')]\n",
    "\n",
    "# add object id\n",
    "segm_and_prediction_df['object_id'] = segm_and_prediction_df['img_id'].astype(str) + '_' + segm_and_prediction_df['index'].astype(str)\n",
    "\n",
    "# Create list of file paths from the 'file_paths' column\n",
    "#filepaths = segm_and_prediction_df['file_paths'].tolist()\n",
    "\n",
    "# delete some columns\n",
    "columns_to_delete = ['temperature', 'mean_raw', 'std_raw', 'mean', 'std', 'x', 'y', 'w', 'h', 'saved', 'bound_box_x', 'bound_box_y', 'full_path', 'img_id', 'index']\n",
    "segm_and_prediction_df.drop(columns_to_delete, axis=1, inplace=True)\n",
    "\n",
    "# adjust header names\n",
    "segm_and_prediction_df = segm_and_prediction_df.rename(columns={\n",
    "    'pressure [dbar]' : 'object_pressure',\n",
    "    'date': 'object_date',\n",
    "    'time' : 'object_time',\n",
    "    'filename' : 'img_file_name',\n",
    "    'area' : 'object_area',\n",
    "    'esd' : 'object_esd',\n",
    "    'top1' : 'object_annotation_category',\n",
    "    'top2' : 'object_annotation_category_2',\n",
    "    'top3' : 'object_annotation_category_3',\n",
    "    'top4' : 'object_annotation_category_4',\n",
    "    'top5' : 'object_annotation_category_5',\n",
    "    'prob1' : 'object_prob_1',\n",
    "    'prob2' : 'object_prob_2',\n",
    "    'prob3': 'object_prob_3',\n",
    "    'prob4' : 'object_prob_4',\n",
    "    'prob5' : 'object_prob_5'    \n",
    "})\n",
    "#print(segm_and_prediction_df.columns)\n",
    "\n",
    "\n",
    "# function to determine the data type of each column\n",
    "def determine_dtype(dtype):\n",
    "    if pd.api.types.is_numeric_dtype(dtype):\n",
    "        return '[f]' \n",
    "    elif pd.api.types.is_string_dtype(dtype):\n",
    "        return '[t]'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "dtype_row = []\n",
    "for col in segm_and_prediction_df.columns:\n",
    "    dtype_row.append(determine_dtype(segm_and_prediction_df[col].dtype))\n",
    "\n",
    "# insert row\n",
    "segm_and_prediction_df.loc[0] = dtype_row\n",
    "\n",
    "# reset index to maintain order\n",
    "segm_and_prediction_df = segm_and_prediction_df.sort_index().reset_index(drop=True)\n",
    "eco_taxa_folder = \"/home/fanny/segmentation_output/EcoTaxa\"\n",
    "os.makedirs(eco_taxa_folder, exist_ok=True)\n",
    "\n",
    "# save everything as tsv file\n",
    "segm_and_prediction_df.to_csv('/home/fanny/segmentation_output/EcoTaxa/ecotaxa_metadata.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipped /home/fanny/segmentation_output/Crops to /home/fanny/segmentation_output/EcoTaxa/ecotaxa_upload_raw.zip\n",
      "Folders have been zipped\n"
     ]
    }
   ],
   "source": [
    "# combine data and compress file\n",
    "\n",
    "def zip_data(folder_path, zip_path, extra_file=None):\n",
    "    \"\"\"\n",
    "    Zips a folder and optionally includes an extra file inside the zip.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): The folder to zip.\n",
    "        zip_path (str): The destination zip file path.\n",
    "        extra_file (str, optional): metadata file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path) or not os.path.isdir(folder_path):\n",
    "        print(f\"Warning: Folder {folder_path} does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Move meta data file into the folder before zipping\n",
    "    if extra_file and os.path.exists(extra_file):\n",
    "        shutil.move(extra_file, os.path.join(folder_path, os.path.basename(extra_file)))\n",
    "\n",
    "    # Create ZIP archive\n",
    "    shutil.make_archive(zip_path.replace(\".zip\", \"\"), 'zip', folder_path)\n",
    "    print(f\"Zipped {folder_path} to {zip_path}\")\n",
    "\n",
    "\n",
    "deconv_crops_folder = '/home/fanny/segmentation_output/Deconv_crops'\n",
    "raw_crops_folder = '/home/fanny/segmentation_output/Crops'\n",
    "metadata = '/home/fanny/segmentation_output/EcoTaxa/ecotaxa_metadata.tsv'\n",
    "\n",
    "# zip paths\n",
    "zip_path_deconv = \"/home/fanny/segmentation_output/EcoTaxa/ecotaxa_upload_deconv.zip\"\n",
    "zip_path_raw = \"/home/fanny/segmentation_output/EcoTaxa/ecotaxa_upload_raw.zip\"\n",
    "\n",
    "#zip_data(deconv_crops_folder, zip_path_deconv, metadata)\n",
    "zip_data(raw_crops_folder, zip_path_raw, metadata)\n",
    "\n",
    "print(\"Folders have been zipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mET_upload\u001b[0;34m(project_id, folder_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m login_cmd \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyecotaxa\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogin\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241m.\u001b[39mrun(login_cmd, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogged into EcoTaxa.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subprocess' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror during upload: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#ET_upload(15753, zip_path_deconv)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mET_upload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m15753\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_path_raw\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mET_upload\u001b[0;34m(project_id, folder_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mrun(cmd, check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccessfully uploaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to project \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43msubprocess\u001b[49m\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror during upload: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subprocess' is not defined"
     ]
    }
   ],
   "source": [
    "# EcoTaxa upload\n",
    "zip_path_deconv = \"/home/fanny/segmentation_output/EcoTaxa/ecotaxa_upload_deconv.zip\"\n",
    "zip_path_raw = \"/home/fanny/segmentation_output/EcoTaxa/ecotaxa_upload_raw.zip\"\n",
    "\n",
    "\n",
    "def ET_upload(project_id, folder_path):\n",
    "    #Uploads a zip file to an EcoTaxa project using terminal commands.\n",
    "    try:\n",
    "        #login to ET project\n",
    "        login_cmd = [\"pyecotaxa\", \"login\"]\n",
    "        subprocess.run(login_cmd, check=True)\n",
    "        print(\"logged into EcoTaxa.\")\n",
    "        \n",
    "        # upload file\n",
    "        cmd = ['pyecotaxa', 'push', '--to', str(project_id), folder_path]\n",
    "        subprocess.run(cmd, check=True)\n",
    "        print(f\"successfully uploaded {folder_path} to project {project_id}.\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"error during upload: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ET_upload(15753, zip_path_deconv)\n",
    "ET_upload(15753, zip_path_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecotaxa.remote import Remote\n",
    "\n",
    "ecotaxa_client = Remote()\n",
    "\n",
    "# Login\n",
    "ecotaxa_client.login()\n",
    "\n",
    "ecotaxa_client.push(15753, '/home/fanny/segmentation_output/EcoTaxa/ecotaxa_upload.zip')\n",
    "\n",
    "print(dir(ecotaxa_client))\n",
    "\n",
    "# in terminal:\n",
    "#pyecotaxa login\n",
    "#pyecotaxa push --to 15753 --validate /home/fanny/segmentation_output/EcoTaxa/ecotaxa_upload.zip    # --validate validates archives locally before uploading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fanny/anaconda3/envs/ImageProcessing/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'zip_path_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     zip_path_deconv \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/fanny/M181-2_output_test/M181-117-1_CTD-038_00°00S-009°00W_20220505-0250/EcoTaxa/ecotaxa_upload_deconv.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Upload the zipped files to EcoTaxa\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m ET_upload(\u001b[38;5;241m15753\u001b[39m, \u001b[43mzip_path_raw\u001b[49m) \n\u001b[1;32m     32\u001b[0m ET_upload(\u001b[38;5;241m15862\u001b[39m, zip_path_deconv)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'zip_path_raw' is not defined"
     ]
    }
   ],
   "source": [
    "from EcoTaxa_preparation import ET_upload\n",
    "import logging\n",
    "import os\n",
    "from pyecotaxa.remote import Remote\n",
    "\n",
    "# #login to ET project\n",
    "#     login_cmd = [\"pyecotaxa\", \"login\"]\n",
    "#     subprocess.run(login_cmd, check=True)\n",
    "\n",
    "def ET_upload(project_id, folder_path): #Uploads a zip file to an EcoTaxa project using terminal commands. \n",
    "    # Upload file\n",
    "    # cmd = ['pyecotaxa', 'push', '--to', str(project_id), folder_path]\n",
    "    # result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "\n",
    "    remote = Remote()\n",
    "\n",
    "    remote.current_user()\n",
    "\n",
    "    remote.push([(folder_path, project_id)])\n",
    "    \n",
    "    logging.info(f\"Successfully uploaded {folder_path} to project {project_id}.\")\n",
    "    logging.info(f\"Upload output: {result.stdout}\")\n",
    "    logging.info(f\"Upload errors: {result.stderr}\")\n",
    "\n",
    "\n",
    " # Define the paths to the zipped files\n",
    "    zip_path_raw = '/home/fanny/M181-2_output_test/M181-117-1_CTD-038_00°00S-009°00W_20220505-0250/EcoTaxa/ecotaxa_upload_raw.zip'\n",
    "    zip_path_deconv = '/home/fanny/M181-2_output_test/M181-117-1_CTD-038_00°00S-009°00W_20220505-0250/EcoTaxa/ecotaxa_upload_deconv.zip'\n",
    "\n",
    "# Upload the zipped files to EcoTaxa\n",
    "ET_upload(15753, zip_path_raw) \n",
    "ET_upload(15862, zip_path_deconv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classlist = ['acantharia_protist',\n",
    "'acantharia_protist_big_center',\n",
    "'acantharia_protist_halo',\n",
    "'amphipods',\n",
    "'appendicularian_fritillaridae',\n",
    "'appendicularian_s_shape',\n",
    "'appendicularian_slight_curve',\n",
    "'appendicularian_straight',\n",
    "'artifacts',\n",
    "'artifacts_edge',\n",
    "'chaetognath_non_sagitta',\n",
    "'chaetognath_other',\n",
    "'chaetognath_sagitta',\n",
    "'chordate_type1',\n",
    "'copepod_calanoid',\n",
    "'copepod_calanoid_eggs',\n",
    "'copepod_calanoid_eucalanus',\n",
    "'copepod_calanoid_flatheads',\n",
    "'copepod_calanoid_frillyAntennae',\n",
    "'copepod_calanoid_large',\n",
    "'copepod_calanoid_large_side_antennatucked',\n",
    "'copepod_calanoid_octomoms',\n",
    "'copepod_calanoid_small_longantennae',\n",
    "'copepod_cyclopoid_copilia',\n",
    "'copepod_cyclopoid_oithona',\n",
    "'copepod_cyclopoid_oithona_eggs',\n",
    "'copepod_other',\n",
    "'crustacean_other',\n",
    "'ctenophore_cestid',\n",
    "'ctenophore_cydippid_no_tentacles',\n",
    "'ctenophore_cydippid_tentacles',\n",
    "'ctenophore_lobate',\n",
    "'decapods',\n",
    "'detritus_blob',\n",
    "'detritus_filamentous',\n",
    "'detritus_other',\n",
    "'diatom_chain_string',\n",
    "'diatom_chain_tube',\n",
    "'echinoderm_larva_pluteus_brittlestar',\n",
    "'echinoderm_larva_pluteus_early',\n",
    "'echinoderm_larva_pluteus_typeC',\n",
    "'echinoderm_larva_pluteus_urchin',\n",
    "'echinoderm_larva_seastar_bipinnaria',\n",
    "'echinoderm_larva_seastar_brachiolaria',\n",
    "'echinoderm_seacucumber_auricularia_larva',\n",
    "'echinopluteus',\n",
    "'ephyra',\n",
    "'euphausiids',\n",
    "'euphausiids_young',\n",
    "'fecal_pellet',\n",
    "'fish_larvae_deep_body',\n",
    "'fish_larvae_leptocephali',\n",
    "'fish_larvae_medium_body',\n",
    "'fish_larvae_myctophids',\n",
    "'fish_larvae_thin_body',\n",
    "'fish_larvae_very_thin_body',\n",
    "'heteropod',\n",
    "'hydromedusae_aglaura',\n",
    "'hydromedusae_bell_and_tentacles',\n",
    "'hydromedusae_h15',\n",
    "'hydromedusae_haliscera',\n",
    "'hydromedusae_haliscera_small_sideview',\n",
    "'hydromedusae_liriope',\n",
    "'hydromedusae_narco_dark',\n",
    "'hydromedusae_narco_young',\n",
    "'hydromedusae_narcomedusae',\n",
    "'hydromedusae_other',\n",
    "'hydromedusae_partial_dark',\n",
    "'hydromedusae_shapeA',\n",
    "'hydromedusae_shapeA_sideview_small',\n",
    "'hydromedusae_shapeB',\n",
    "'hydromedusae_sideview_big',\n",
    "'hydromedusae_solmaris',\n",
    "'hydromedusae_solmundella',\n",
    "'hydromedusae_typeD',\n",
    "'hydromedusae_typeD_bell_and_tentacles',\n",
    "'hydromedusae_typeE',\n",
    "'hydromedusae_typeF',\n",
    "'invertebrate_larvae_other_A',\n",
    "'invertebrate_larvae_other_B',\n",
    "'jellies_tentacles',\n",
    "'polychaete',\n",
    "'protist_dark_center',\n",
    "'protist_fuzzy_olive',\n",
    "'protist_noctiluca',\n",
    "'protist_other',\n",
    "'protist_star',\n",
    "'pteropod_butterfly',\n",
    "'pteropod_theco_dev_seq',\n",
    "'pteropod_triangle',\n",
    "'radiolarian_chain',\n",
    "'radiolarian_colony',\n",
    "'shrimp-like_other',\n",
    "'shrimp_caridean',\n",
    "'shrimp_sergestidae',\n",
    "'shrimp_zoea',\n",
    "'siphonophore_calycophoran_abylidae',\n",
    "'siphonophore_calycophoran_rocketship_adult',\n",
    "'siphonophore_calycophoran_rocketship_young',\n",
    "'siphonophore_calycophoran_sphaeronectes',\n",
    "'siphonophore_calycophoran_sphaeronectes_stem',\n",
    "'siphonophore_calycophoran_sphaeronectes_young',\n",
    "'siphonophore_other_parts',\n",
    "'siphonophore_partial',\n",
    "'siphonophore_physonect',\n",
    "'siphonophore_physonect_young',\n",
    "'stomatopod',\n",
    "'tornaria_acorn_worm_larvae',\n",
    "'trichodesmium_bowtie',\n",
    "'trichodesmium_multiple',\n",
    "'trichodesmium_puff',\n",
    "'trichodesmium_tuft',\n",
    "'trochophore_larvae',\n",
    "'tunicate_doliolid',\n",
    "'tunicate_doliolid_nurse',\n",
    "'tunicate_partial',\n",
    "'tunicate_salp',\n",
    "'tunicate_salp_chains',\n",
    "'unknown_blobs_and_smudges',\n",
    "'unknown_sticks',\n",
    "'unknown_unclassified']\n",
    "\n",
    "classlist_mapping = {}\n",
    "\n",
    "for item in classlist:\n",
    "    new_name = input(f\"Enter new name for {item}: \")\n",
    "    classlist_mapping[item] = new_name\n",
    "\n",
    "print(classlist_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "image_folder = '/home/fanny/segmentation_output/Deconv_crops'\n",
    "\n",
    "def is_valid_image(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img.verify()  # Verify integrity\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Corrupt image detected: {image_path} - {e}\")\n",
    "        return False\n",
    "    \n",
    "valid_images = []\n",
    "for filename in os.listdir(image_folder):\n",
    "    image_path = os.path.join(image_folder, filename)\n",
    "    \n",
    "    # Ensure it's a valid image\n",
    "    if is_valid_image(image_path):\n",
    "        valid_images.append(image_path)\n",
    "\n",
    "print(f\"Found {len(valid_images)} valid images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EcoTaxa upload\n",
    "\n",
    "from segmenter import run_segmenter\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from pandas.errors import EmptyDataError\n",
    "import numpy as np\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "def gen_crop_df(path:str, small:bool, size_filter:int = 0):\n",
    "    \"\"\"\n",
    "    A function to generate a DataFrame from a directory of CSV files, with options to filter out small objects.\n",
    "    Parameters:\n",
    "    path (str): The path to the directory containing the CSV files.\n",
    "    small (bool): A flag indicating whether to filter out small objects.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The concatenated and processed DataFrame with additional columns for analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def area_to_esd(area: float) -> float:\n",
    "        pixel_size = 13.5*2 #in µm/pixel @ 2560x2560 \n",
    "        return 2 * np.sqrt(area * pixel_size**2 / np.pi)\n",
    "\n",
    "    # Function to concatenate directory and filename\n",
    "    def join_strings(dir, filename):\n",
    "        return os.path.join(dir, filename)\n",
    "\n",
    "    directory = os.path.dirname(path)\n",
    "    directory = os.path.join(directory,'Data')\n",
    "\n",
    "    files = [os.path.join(path, file) for file in sorted(os.listdir(path)) if file.endswith(\".csv\")]\n",
    "    dataframes = []\n",
    "    empty_file_counter = 0\n",
    "    id = 1\n",
    "    for file in tqdm(files):\n",
    "        try:\n",
    "            df = pd.read_csv(file, delimiter=\",\", header=None, index_col=None)\n",
    "            if len(df.columns) == 44:\n",
    "                df.insert(0,'',id)            \n",
    "                dataframes.append(df)\n",
    "                id+=1\n",
    "            else:\n",
    "                continue\n",
    "        except EmptyDataError:\n",
    "            empty_file_counter += 1\n",
    "            print(f\"File {file} is empty\")\n",
    "\n",
    "    df = pd.concat(dataframes, ignore_index=True)\n",
    "    headers = [\"img_id\",\"index\", \"filename\", \"mean_raw\", \"std_raw\", \"mean\", \"std\", \"area\", \"x\", \"y\", \"w\", \"h\", \n",
    "               \"saved\", \"object_bound_box_w\", \"object_bound_box_h\", \"bound_box_x\", \"bound_box_y\", \"object_circularity\", \"object_area_exc\", \n",
    "               \"object_area_rprops\", \"object_%area\", \"object_major_axis_len\", \"object_minor_axis_len\", \"object_centroid_y\", \"object_centroid_x\", \n",
    "               \"object_convex_area\", \"object_min_intensity\", \"object_max_intensity\", \"object_mean_intensity\", \"object_int_density\", \"object_perimeter\", \n",
    "               \"object_elongation\", \"object_range\", \"object_perim_area_excl\", \"object_perim_major\", \"object_circularity_area_excl\", \"object_angle\", \n",
    "               \"object_boundbox_area\", \"object_eccentricity\", \"object_equivalent_diameter\", \"object_euler_nr\", \"object_extent\", \n",
    "               \"object_local_centroid_col\", \"object_local_centroid_row\", \"object_solidity\"\n",
    "]\n",
    "    df.columns = headers\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.drop(\"index\", axis=1, inplace=True)\n",
    "\n",
    "    if not small:\n",
    "        df = df[df[\"saved\"] == 1]\n",
    "    df_unique = df.drop_duplicates(subset=['img_id'])\n",
    "    \n",
    "    #df.drop(\"saved\", axis=1, inplace=True)\n",
    "\n",
    "    # Split the 'filename' column\n",
    "    split_df = df['filename'].str.split('_', expand=True)\n",
    "    if small:# bug fix for segmenter where small objects are saved with _mask.png extension instead of .png: needs to be fixed if segmenter is fixed\n",
    "        headers = [\"date\", \"time\", \"pressure\", \"temperature\", \"index\", \"mask_ext\"]\n",
    "        split_df.columns = headers\n",
    "        split_df.drop(\"mask_ext\", axis=1, inplace=True)\n",
    "    else:\n",
    "        headers = [\"date-time\", \"pressure\", \"temperature\", \"index\"]#, 'drop']\n",
    "        split_df.columns = headers\n",
    "        #split_df.drop(\"drop\", axis=1, inplace=True)\n",
    "    \n",
    "    # split date-time\n",
    "    split_df[['date', 'time']] = split_df['date-time'].str.split('-', expand=True)\n",
    "    split_df.drop(columns=['date-time'], inplace=True)\n",
    "\n",
    "    split_df['pressure'] = split_df['pressure'].str.replace('bar', '', regex=False).astype(float)\n",
    "    split_df['temperature'] = split_df['temperature'].str.replace('C', '', regex=False).astype(float)\n",
    "    split_df['index'] = split_df['index'].str.replace('.png', '', regex=False).astype(int)\n",
    "    \n",
    "    # Concatenate the new columns with the original DataFrame\n",
    "    df = pd.concat([split_df, df], axis=1)\n",
    "\n",
    "    # Extend the original 'filename' column\n",
    "    df['full_path'] = df.apply(lambda x: join_strings(directory, x['filename']), axis=1)\n",
    "    #df = df.drop('filename', axis=1)\n",
    "\n",
    "    df['esd'] = df['area'].apply(area_to_esd).round(2)\n",
    "    df['pressure'] = (df['pressure']-1)*10\n",
    "    df.rename(columns={'pressure': 'pressure [dbar]'}, inplace=True)\n",
    "\n",
    "    # Sort the DataFrame by the 'date-time' column\n",
    "    df = df.sort_values(by=['date', 'time','index'], ascending=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #filter the df for objects where 1 dimension is larger than ca. 1mm\n",
    "    df = df[(df['w'] > size_filter) | (df['h'] > size_filter)]\n",
    "    df_unique = df.drop_duplicates(subset=['img_id'])\n",
    "    print(f'{empty_file_counter} files were empty and were dropped; Number of uniue images: {len(df_unique)}')\n",
    "\n",
    "    return df\n",
    "\n",
    "file_path = '/home/fanny/segmentation_output/Data'\n",
    "segmentation_df = gen_crop_df(file_path, False)\n",
    "\n",
    "\n",
    "\n",
    "#Loads prediction and mapping data, processes class names, and updates object annotation status\n",
    "def prepare_prediction_data(prediction_csv, mapping_csv, sep=\"\\t\"):    \n",
    "    # Load CSV files\n",
    "    prediction_df = pd.read_csv(prediction_csv)\n",
    "    polytaxo_classes_df = pd.read_csv(mapping_csv, sep=sep)\n",
    "\n",
    "    # Add annotation status\n",
    "    prediction_df['object_annotation_status'] = 'predicted'\n",
    "\n",
    "    # Create mapping dictionary\n",
    "    mapping_dict = dict(zip(\n",
    "        polytaxo_classes_df[\"Dataset Class NamePolyTaxo Description\"],\n",
    "        polytaxo_classes_df[\"PolyTaxo Description\"]\n",
    "    ))\n",
    "\n",
    "    # Columns to update\n",
    "    columns_to_replace = [\"top1\", \"top2\", \"top3\", \"top4\", \"top5\"]\n",
    "\n",
    "    # Define regex pattern to split on space, semicolon, colon, or slash\n",
    "    split_pattern = r\"[ ;:/]\"\n",
    "\n",
    "    # Replace values using mapping_dict, extract first word, and replace underscores with spaces\n",
    "    prediction_df[columns_to_replace] = prediction_df[columns_to_replace].replace(mapping_dict).apply(\n",
    "        lambda col: col.astype(str).apply(\n",
    "            lambda x: re.split(split_pattern, x)[0].replace(\"_\", \" \") if pd.notna(x) else x\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return prediction_df  # Return the processed DataFrame\n",
    "\n",
    "\n",
    "def combine_segmentation_and_prediction(segmentation_df, prediction_df):\n",
    "        \n",
    "    # Sort both DataFrames by 'filename'\n",
    "    segmentation_df_sorted = segmentation_df.sort_values(by='filename').reset_index(drop=True)\n",
    "    prediction_df_sorted = prediction_df.sort_values(by='filename').reset_index(drop=True)\n",
    "\n",
    "    # Concatenate data frames\n",
    "    combined_df = pd.concat([segmentation_df_sorted, prediction_df_sorted], axis=1)\n",
    "    \n",
    "    # Remove duplicate columns (keeping the first occurrence)\n",
    "    combined_df = combined_df.loc[:, ~combined_df.columns.duplicated(keep='first')]\n",
    "\n",
    "    # Add object ID column\n",
    "    combined_df['object_id'] = combined_df['img_id'].astype(str) + '_' + combined_df['index'].astype(str)\n",
    "\n",
    "    # Define columns to delete\n",
    "    columns_to_delete = [\n",
    "        'temperature', 'mean_raw', 'std_raw', 'mean', 'std', 'x', 'y', 'w', 'h', \n",
    "        'saved', 'bound_box_x', 'bound_box_y', 'full_path', 'img_id', 'index'\n",
    "    ]\n",
    "    # Remove unwanted columns if they exist\n",
    "    combined_df.drop(columns=[col for col in columns_to_delete if col in combined_df.columns], axis=1, inplace=True)\n",
    "\n",
    "    # Adjust header names\n",
    "    rename_mapping = {\n",
    "        'pressure [dbar]': 'object_pressure',\n",
    "        'date': 'object_date',\n",
    "        'time': 'object_time',\n",
    "        'filename': 'img_file_name',\n",
    "        'area': 'object_area',\n",
    "        'esd': 'object_esd',\n",
    "        'top1': 'object_annotation_category',\n",
    "        'top2': 'object_annotation_category_2',\n",
    "        'top3': 'object_annotation_category_3',\n",
    "        'top4': 'object_annotation_category_4',\n",
    "        'top5': 'object_annotation_category_5',\n",
    "        'prob1': 'object_prob_1',\n",
    "        'prob2': 'object_prob_2',\n",
    "        'prob3': 'object_prob_3',\n",
    "        'prob4': 'object_prob_4',\n",
    "        'prob5': 'object_prob_5'\n",
    "    }\n",
    "    combined_df.rename(columns=rename_mapping, inplace=True)\n",
    "\n",
    "    return combined_df  # Return the processed DataFrame\n",
    "\n",
    "\n",
    "\n",
    "# combine data and compress file\n",
    "def zip_data(folder_path, zip_path, extra_file=None):\n",
    "    \"\"\"\n",
    "    Zips a folder and optionally includes an extra file inside the zip.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path (str): The folder to zip.\n",
    "        zip_path (str): The destination zip file path.\n",
    "        extra_file (str, optional): metadata file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path) or not os.path.isdir(folder_path):\n",
    "        print(f\"Warning: Folder {folder_path} does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Move meta data file into the folder before zipping\n",
    "    if extra_file and os.path.exists(extra_file):\n",
    "        shutil.move(extra_file, os.path.join(folder_path, os.path.basename(extra_file)))\n",
    "\n",
    "    # Create ZIP archive\n",
    "    shutil.make_archive(zip_path.replace(\".zip\", \"\"), 'zip', folder_path)\n",
    "    print(f\"Zipped {folder_path} to {zip_path}\")\n",
    "\n",
    "\n",
    "deconv_crops_folder = '/home/fanny/segmentation_output/Deconv_crops'\n",
    "raw_crops_folder = '/home/fanny/segmentation_output/Crops'\n",
    "metadata = '/home/fanny/segmentation_output/EcoTaxa/ecotaxa_metadata.tsv'\n",
    "\n",
    "# zip paths\n",
    "zip_path_deconv = \"/home/fanny/segmentation_output/EcoTaxa/ecotaxa_upload_deconv.zip\"\n",
    "zip_path_raw = \"/home/fanny/segmentation_output/EcoTaxa/ecotaxa_upload_raw.zip\"\n",
    "\n",
    "#zip_data(deconv_crops_folder, zip_path_deconv, metadata)\n",
    "zip_data(raw_crops_folder, zip_path_raw, metadata)\n",
    "\n",
    "print(\"Folders have been zipped\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ET_upload(project_id, folder_path): #Uploads a zip file to an EcoTaxa project using terminal commands.\n",
    "    try:\n",
    "        #login to ET project\n",
    "        login_cmd = [\"pyecotaxa\", \"login\"]\n",
    "        subprocess.run(login_cmd, check=True)\n",
    "        \n",
    "        # upload file\n",
    "        cmd = ['pyecotaxa', 'push', '--to', str(project_id), folder_path]\n",
    "        subprocess.run(cmd, check=True)\n",
    "        print(f\"successfully uploaded {folder_path} to project {project_id}.\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"error during upload: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ET_upload(15862, zip_path_deconv)\n",
    "ET_upload(15753, zip_path_raw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImageProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
